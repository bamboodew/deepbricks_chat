from openai import OpenAI
import os
from dotenv import load_dotenv
import tiktoken  # ç”¨äºè®¡ç®— token æ•°

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

API_KEY = os.environ.get("DEEPBRICKS_API_KEY")
BASE_URL = "https://api.deepbricks.ai/v1/"

if not API_KEY:
    raise ValueError("è¯·è®¾ç½® DEEPBRICKS_API_KEY ç¯å¢ƒå˜é‡")

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# åˆå§‹åŒ–å¯¹è¯å†å²
conversation_history = []

# åˆå§‹åŒ– token ç¼–ç å™¨
encoding = tiktoken.get_encoding("cl100k_base")  # æ ¹æ®å…·ä½“æ¨¡å‹é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨


def count_tokens(text):
    """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    return len(encoding.encode(text))


def chat_with_ai(user_input):
    global conversation_history

    # è®¡ç®—ç”¨æˆ·è¾“å…¥çš„ token æ•°
    user_tokens = count_tokens(user_input)

    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²
    conversation_history.append({"role": "user", "content": user_input})

    try:
        # åˆ›å»ºèŠå¤©å®Œæˆ
        completion = client.chat.completions.create(
            model="GPT-4o-mini", messages=conversation_history
        )

        # è·å–AIçš„å›å¤
        ai_response = completion.choices[0].message.content

        # è®¡ç®— AI å›å¤çš„ token æ•°
        ai_tokens = count_tokens(ai_response)

        # å°† AI çš„å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²
        conversation_history.append({"role": "assistant", "content": ai_response})

    except Exception as e:
        ai_response = f"å‘ç”Ÿé”™è¯¯: {str(e)}"
        ai_tokens = count_tokens(ai_response)

    return ai_response, user_tokens, ai_tokens


# ä¸»å¾ªç¯
print("å¼€å§‹å¯¹è¯ (è¾“å…¥ 'é€€å‡º', 'exit', æˆ– 'quit' ç»“æŸå¯¹è¯)ï¼š")
while True:
    user_input = input("ğŸ˜Š: ")
    if user_input.lower() in ["é€€å‡º", "exit", "quit"]:
        print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå¯¹è¯å·²ç»“æŸã€‚")
        break

    response, user_tokens, ai_tokens = chat_with_ai(user_input)
    print(f"ğŸ¤–: {response} [è¾“å…¥ Token: {user_tokens}, è¾“å‡º Token: {ai_tokens}]")
    print("-" * 50)
